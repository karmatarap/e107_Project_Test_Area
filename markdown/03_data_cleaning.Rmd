# Data cleaning

## Read the data

In the following, we define a function that reads the corpus which has detailed annotations: for each tweet it contains detailed information about the adverse event:

```{r 03_parse_diego_data}
#' Parse downloaded annotated data
#'
#' @description Parses the data from the DIEGO labs. Depending on the argument 
#' it reads in the training or test data. It joins the tweets with the 
#' annotations. Empty annotations means no adverse reaction.
#'
#' @param type = c("train", "test")
#'
#' @return data frame
#'
#' @examples parse_diego_data("train")
#' @import dplyr, magrittr, readr, stringr
parse_diego_data <- function(type) {
    # Load data
    annotation_file <-
        ifelse(type == "train", 
               "../data/download_tweets/train_tweet_annotations.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/test_tweet_annotations.tsv",
                      stop("type should be either 'train' or 'test'"
                      ))
               )
    
    tweets_file <- 
        ifelse(type == "train",
               "../data/download_tweets/full_train_tweet_ids.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/full_test_tweet_ids.tsv",
                      stop("type should be either 'train' or 'test'"))
               )
    
    
    # Read annotations and rename columns
    annotations <- 
        read_tsv(annotation_file,
                 col_names = FALSE,
                 col_types = c("ciicccc")
        ) %>% 
        rename(text_id = X1,
               start_offset = X2,
               end_offset = X3,
               semantic_type = X4,
               annotated_text = X5,
               related_drug = X6,
               target_drug = X7
        )
    
    # Parse tweets
    tweets <- 
        read_lines(tweets_file) %>% 
        str_split(., "\t") %>%
        as.data.frame() %>% 
        as.matrix() %>% 
        t %>% 
        as.data.frame() %>% 
        `rownames<-`(., NULL) %>% 
        rename(tweet_id = V1,
               user_id = V2,
               text_id = V3,
               tweet_text = V4
        )
    
    # Read in the ADR lexicon and rename
    adr_lexicon <- read_tsv("../data/download_tweets/ADR_lexicon.tsv",
                            skip = 21, 
                            col_names = FALSE) %>% 
        rename(concept_id = X1,
               concept_name = X2,
               source = X3)
    
    # Join tweets and annotations
    result <- 
        suppressWarnings(left_join(tweets, annotations, by = "text_id"))
    
    # Join result with ADR lexicon
    result <- left_join(result, 
                        adr_lexicon,
                        by = c("annotated_text" = "concept_name"))
    
    # Select distinct tweets
    result %<>% distinct(tweet_id)
    
    result
}
```

Next, we define a similar function that reads in the binary annotated file:

```{r 03_parse_binary_annotations}
#' Parse binary data
#'
#' @description Parses the data with binary annotationÂ¨
#'
#' @return data frame
#'
#' @examples parse_binary_data()
#' @import dplyr, purrr, readr, stringr

parse_binary_data <- function(type) {
    # Load data
    tweets_file <- "../data/download_tweets/binary_tweets_downloaded.tsv"
    
    # Parse tweets: bad character "\r" has to be removed before the string
    # can be split
    individual_tweets <- 
        read_file(tweets_file) %>% 
        str_replace_all("\r", "") %>% 
        str_split(., "\n") %>%
        unlist()
    
    # Split the tweets and have four output columns
    # Revalue is_AE column (needed for caret to work with some ML algos)
    tweets_frame <- 
        individual_tweets %>% 
        str_split_fixed("\t", 4) %>% 
        as.data.frame() %>% 
        rename(tweet_id = V1,
               user_id = V2,
               is_AE = V3,
               tweet_text = V4
        ) %>% 
        filter(is_AE != "") %>% 
        mutate(is_AE = as.factor(plyr::revalue(is_AE, 
                                               c("0" = "No", "1" = "Yes"))
                                 ))
    
    tweets_frame
}
```



```{r}

#setwd("/Users/bear/Studies/Harvard/Data_Science/project/e107_Project_Test_Area/markdown")

# Read in the training data 
source("../r/helper_functions/parse_diego_data.R")

train.data <- parse_diego_data("train")

#converting factors to string
train.data[ ] <- lapply(train.data, as.character)


# Libraries used
library(qdap)
library(qdapDictionaries)
library(rafalib)
library(dplyr)
library(tm)
library(stringr)
library(rvest)    #web scraping
library(hunspell) #spelling dictionary, installed with brew on MacOS
library(hash)
set.seed(999)

# Investigate a subsection of the data to see what kind of text we are dealing with 
x <- train.data[sample(nrow(train.data),5), ]
x
```

# Drug names

## Downloaded drug names from https://www.nlm.nih.gov/research/umls/rxnorm/docs/rxnormfiles.html

These are stored as RXNCONSO.RFF. 
Create a list so we can identify tweets containing a known drug name, as well as ignore them from spell checking


```{r}

# Read drug names 
rx.conso <- read.delim(file = '../data/dictionaries/RXNCONSO.RRF', sep='|',header = F, stringsAsFactors=F)


# Column names
rx.colstr <- "RXCUI LAT TS LUI STT SUI SPREF RXAUI SAUI SCUI SDUI SAB TTY CODE STR SRL SUPPRESS CVF"

# Assign column names to data frame
names(rx.conso) <- rx.colstr %>%
  tolower %>%
  strsplit(split=' ') %>% 
  unlist

# Subset a list of drug brand names and generics
# convert to lower for easier lookup
drug.list <- rx.conso %>%
  subset((tty %in% c('BN','IN'))) %>%
  select(tty,code,str) %>%
  mutate(str=tolower(str))


```


# Slang words

Scrape these from http://www.netlingo.com/acronyms.php.

Identify a list of slang words, so we can translate them into their english counterpart before passing to the spellchecker 

```{r}


url <- "http://www.netlingo.com/acronyms.php"

contents <- read_html(url)

#write(contents, file = "../data/dictionaries/slang.txt")

#contents <- read.table("../data/dictionaries/slang.txt")

#lapply(contents, write, "../data/dictionaries/slang_words", append=TRUE, ncolumns=1000)

# Using the chrome selector gadget we identified the following attributes containing the dictionary: .list_box3 li, .list_box3 span
words <- contents %>%
  html_nodes(".list_box3 span") %>%
  html_text() %>% tolower 

descriptions <- contents %>%
  html_nodes(".list_box3 li") %>%
  html_text() %>% tolower 

slang.lookup <- data.frame(words, descriptions, stringsAsFactors = FALSE)

# Cleaning out the descriptions, as it contains the slang words also
# Also removing slangs that can have multiple interpretations
# Remove the null key
slang.lookup <- slang.lookup %>% 
  mutate(decode = substring(descriptions,nchar(words)+1)) %>% 
  filter(!grepl(",|-or-",descriptions)) %>%
  filter(words != "")


```







```{r}

# Adding some attributes to the training dataset


train.data.new <- train.data %>% 
  mutate(vect    = strsplit(tweet_text,split=" "), 
         hashtag = str_extract_all(tweet_text, "#[a-zA-Z]+"),
         URL     = grepl("http[[:alnum:][:punct:]]*", tweet_text),
         ae_term = substr(tweet_text,start_offset,end_offset)) 


# get drug names
drugs <- sapply(train.data.new$vect,function(x){intersect(x,drug.list$str)})

# get list emoticon meanings
data("emoticon")
emoticons <- sapply(train.data.new$vect, function(x){emoticon$meaning[match(x,emoticon$emoticon)]})

# retrieve drug names and emotions
train.data.new <- train.data.new %>% 
  mutate(rown = row_number(),
         drugs = drugs,
         emoticons = emoticons)

#place emoticons and slangs into a lookup for substitution
h_emoticons <- hash(emoticon$emoticon, emoticon$meaning)
h_slang <- hash(slang.lookup$words, slang.lookup$decode)


# Create corpus
tweet.corpus <- Corpus(VectorSource(train.data.new$tweet_text))

# Convert all to lower case
tweet.corpus <- tm_map(tweet.corpus, content_transformer(tolower))

# Create some functions to help with the cleaning
# Coding urls to url so we can group them and use as a feature in our model
subURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "url", x)
subURL("ofdmfl http://dfdsofdf") == "ofdmfl url"

# Remove twitter handles references
removeHandles <- function(x) gsub("@[[:alnum:][:punct:]]+", "", x)
removeHandles("@so_fd45 this is a test") == " this is a test"

# No words have three consecutive letters of the same character, so compressing to 2
contractLength <- function(x) gsub("([:lower:])\\1+","\\1\\1", x)
contractLength("coooool") == "cool"


# Start by contracting the length
tweet.corpus <- tm_map(tweet.corpus, content_transformer(contractLength))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(removeHandles))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(subURL))


for (tweet in seq(tweet.corpus)){
  for (word in str_split(tweet.corpus[[tweet]],"[^a-z0-9_]")[[1]]){
    
    # ignore if the word is a drug
    if (!word %in% drug.list$str && word !=""){
      
      if (!has.key(word,h_slang) && !has.key(word,h_emoticons)){
         word = ifelse(hunspell_check(word),word,hunspell_suggest(word)[[1]][1])
      }
      else {  
         # if the word is a slang/abb, then convert to english
         word = ifelse(has.key(word,h_slang),h_slang[[word]],word)
      
         # if the word is an emoticon, convert to english
         word = ifelse(has.key(word,h_emoticons),h_emoticons[[word]],word)
      }
    }
  }
}


#cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), stringsAsFactors = FALSE)
# posted comments on SO about this not working

tdm <- TermDocumentMatrix(tweet.corpus,
                          control = list(
                                         removePunctuation = TRUE,
                                         stopwords = TRUE,
                                         stem_words("english")))


pos.score <- tm_term_score(tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

neg.score <- tm_term_score(tdm,terms_in_General_Inquirer_categories("Negativ")) 


cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), 
                             positive = pos.score, 
                             negative = neg.score, 
                             stringsAsFactors = FALSE)



cleaned_tweets <- cleaned_tweets %>% 
  mutate(rown = row_number(), 
         positivity = (positive - negative) / (positive + negative + 1))


cleaned_tweets <- 
  cleaned_tweets %>% 
  inner_join(train.data.new)

save(cleaned_tweets,file="../data/cleaned_tweets.Rda")

```

