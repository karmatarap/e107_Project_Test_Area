# Data cleaning

## Read the data

In the following, we define a function that reads the corpus which has detailed annotations: for each tweet it contains detailed information about the adverse event:

```{r 03_parse_diego_data}
#' Parse downloaded annotated data
#'
#' @description Parses the data from the DIEGO labs. Depending on the argument 
#' it reads in the training or test data. It joins the tweets with the 
#' annotations. Empty annotations means no adverse reaction.
#'
#' @param type = c("train", "test")
#'
#' @return data frame
#'
#' @examples parse_diego_data("train")
#' @import dplyr, magrittr, readr, stringr
parse_diego_data <- function(type) {
    # Load data
    annotation_file <-
        ifelse(type == "train", 
               "../data/download_tweets/train_tweet_annotations.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/test_tweet_annotations.tsv",
                      stop("type should be either 'train' or 'test'"
                      ))
               )
    
    tweets_file <- 
        ifelse(type == "train",
               "../data/download_tweets/full_train_tweet_ids.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/full_test_tweet_ids.tsv",
                      stop("type should be either 'train' or 'test'"))
               )
    
    
    # Read annotations and rename columns
    annotations <- 
        read_tsv(annotation_file,
                 col_names = FALSE,
                 col_types = c("ciicccc")
        ) %>% 
        rename(text_id = X1,
               start_offset = X2,
               end_offset = X3,
               semantic_type = X4,
               annotated_text = X5,
               related_drug = X6,
               target_drug = X7
        )
    
    # Parse tweets
    tweets <- 
        read_lines(tweets_file) %>% 
        str_split(., "\t") %>%
        as.data.frame() %>% 
        as.matrix() %>% 
        t %>% 
        as.data.frame() %>% 
        `rownames<-`(., NULL) %>% 
        rename(tweet_id = V1,
               user_id = V2,
               text_id = V3,
               tweet_text = V4
        )
    
    # Read in the ADR lexicon and rename
    adr_lexicon <- read_tsv("../data/download_tweets/ADR_lexicon.tsv",
                            skip = 21, 
                            col_names = FALSE) %>% 
        rename(concept_id = X1,
               concept_name = X2,
               source = X3)
    
    # Join tweets and annotations
    result <- 
        suppressWarnings(left_join(tweets, annotations, by = "text_id"))
    
    # Join result with ADR lexicon
    result <- left_join(result, 
                        adr_lexicon,
                        by = c("annotated_text" = "concept_name"))
    
    # Select distinct tweets
    result %<>% distinct(tweet_id)
    
    result[ ] <- lapply(result, as.character)
    
    result
}
```

Next, we define a similar function that reads in the binary annotated file:

```{r 03_parse_binary_annotations}
#' Parse binary data
#'
#' @description Parses the data with binary annotationÂ¨
#'
#' @return data frame
#'
#' @examples parse_binary_data()
#' @import dplyr, purrr, readr, stringr

parse_binary_data <- function(type) {
    # Load data
    tweets_file <- "../data/download_tweets/binary_tweets_downloaded.tsv"
    
    # Parse tweets: bad character "\r" has to be removed before the string
    # can be split
    individual_tweets <- 
        read_file(tweets_file) %>% 
        str_replace_all("\r", "") %>% 
        str_split(., "\n") %>%
        unlist()
    
    # Split the tweets and have four output columns
    # Revalue is_AE column (needed for caret to work with some ML algos)
    tweets_frame <- 
        individual_tweets %>% 
        str_split_fixed("\t", 4) %>% 
        as.data.frame() %>% 
        rename(tweet_id = V1,
               user_id = V2,
               is_AE = V3,
               tweet_text = V4
        ) %>% 
        filter(is_AE != "") %>% 
        mutate(is_AE = as.factor(plyr::revalue(is_AE, 
                                               c("0" = "No", "1" = "Yes"))
                                 ))
    tweets_frame[ ] <- lapply(tweets_frame, as.character)
    
    tweets_frame
}
```

Now that we have defined the functions reading in the data, let us take a first look at them to get a feeling:

```{r 03_first_look_at_data}
# Read in the training data 
train.data <- parse_diego_data("train")

# Set seed
set.seed(999)

# Investigate a subsection of the data to see what kind of text we are dealing with 
x <- train.data[sample(nrow(train.data),5), ] %>% tbl_df
str(x)
```

## Drug names

The drug names were downloaded from [here](https://www.nlm.nih.gov/research/umls/rxnorm/docs/rxnormfiles.html).

These are stored as `./data/dictionaries/RXNCONSO.RFF`. 

We create a function that makes a list of drugs so we can identify tweets containing a known drug name, as well as ignore them from spell checking.

```{r 03_create_drug_list}
make_drug_list <- function() {
    # Read drug names 
    rx.conso <- read.delim(file = '../data/dictionaries/RXNCONSO.RRF',
                       sep='|',
                       header = F, 
                       stringsAsFactors = F)
    
    # Column names
    rx.colstr <- "RXCUI LAT TS LUI STT SUI SPREF RXAUI SAUI SCUI SDUI SAB TTY CODE STR SRL SUPPRESS CVF"
    
    # Assign column names to data frame
    names(rx.conso) <- rx.colstr %>%
        tolower %>%
        strsplit(split=' ') %>% 
        unlist
    
    # Subset a list of drug brand names and generics
    # convert to lower for easier lookup
    drug.list <- rx.conso %>%
        subset((tty %in% c('BN','IN'))) %>%
        select(tty, code, str) %>%
        mutate(str = tolower(str))
    
    drug.list
    }

drug.list <- make_drug_list()
```

<em> What does 'BN' and 'IN' mean? We should explain it </em>

## Slang words

The idea in this step is to clean slang expression and convert then to proper english as well as possible. This is done before passing the tweets to the spellchecker.

Slang words were scraped from [here](http://www.netlingo.com/acronyms.php).

Again, we write a function that makes the conversion table:

```{r 03_slang_expressions}
make_slang_lookup <- function() {
    url <- "http://www.netlingo.com/acronyms.php"
    
    contents <- read_html(url)
    
    #write(contents, file = "../data/dictionaries/slang.txt")
    
    #contents <- read.table("../data/dictionaries/slang.txt")
    
    #lapply(contents, write, "../data/dictionaries/slang_words", append=TRUE, ncolumns=1000)
    
    # Using the chrome selector gadget we identified the following attributes containing the dictionary: .list_box3 li, .list_box3 span
    words <- contents %>%
        html_nodes(".list_box3 span") %>%
        html_text() %>% tolower 
    
    descriptions <- contents %>%
        html_nodes(".list_box3 li") %>%
        html_text() %>% tolower 
    
    slang.lookup <- data.frame(words, descriptions, stringsAsFactors = FALSE)
    
    # Cleaning out the descriptions, as it contains the slang words also
    # Also removing slangs that can have multiple interpretations
    # Remove the null key
    slang.lookup <- slang.lookup %>% 
        mutate(decode = substring(descriptions,nchar(words)+1)) %>% 
        filter(!grepl(",|-or-",descriptions)) %>%
        filter(words != "")
}

slang.lookup <- make_slang_lookup()
```

Next, we put every together: we add new attributes to the dataset

+ The **vector of words**: we take the tweet text and split it spaces.
+ We extract the **hastags**
+ We see whether the tweet text contains a **URL**. This could be related to advertisments
+ 

```{r}

# Adding some attributes to the training dataset


train.data.new <- train.data %>% 
  mutate(vect    = strsplit(tweet_text, split=" "), 
         hashtag = str_extract_all(tweet_text, "#[a-zA-Z]+"),
         URL     = grepl("http[[:alnum:][:punct:]]*", tweet_text),
         ae_term = substr(tweet_text,start_offset,end_offset)) 


# get drug names
drugs <- sapply(train.data.new$vect,function(x){intersect(x,drug.list$str)})

# get list emoticon meanings
data("emoticon")
emoticons <- sapply(train.data.new$vect, function(x){emoticon$meaning[match(x,emoticon$emoticon)]})

# retrieve drug names and emotions
train.data.new <- train.data.new %>% 
  mutate(rown = row_number(),
         drugs = drugs,
         emoticons = emoticons)

#place emoticons and slangs into a lookup for substitution
h_emoticons <- hash(emoticon$emoticon, emoticon$meaning)
h_slang <- hash(slang.lookup$words, slang.lookup$decode)


# Create corpus
tweet.corpus <- Corpus(VectorSource(train.data.new$tweet_text))

# Convert all to lower case
tweet.corpus <- tm_map(tweet.corpus, content_transformer(tolower))

# Create some functions to help with the cleaning
# Coding urls to url so we can group them and use as a feature in our model
subURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "url", x)
subURL("ofdmfl http://dfdsofdf") == "ofdmfl url"

# Remove twitter handles references
removeHandles <- function(x) gsub("@[[:alnum:][:punct:]]+", "", x)
removeHandles("@so_fd45 this is a test") == " this is a test"

# No words have three consecutive letters of the same character, so compressing to 2
contractLength <- function(x) gsub("([:lower:])\\1+","\\1\\1", x)
contractLength("coooool") == "cool"


# Start by contracting the length
tweet.corpus <- tm_map(tweet.corpus, content_transformer(contractLength))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(removeHandles))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(subURL))


for (tweet in seq(tweet.corpus)){
  for (word in str_split(tweet.corpus[[tweet]],"[^a-z0-9_]")[[1]]){
    
    # ignore if the word is a drug
    if (!word %in% drug.list$str && word !=""){
      
      if (!has.key(word,h_slang) && !has.key(word,h_emoticons)){
         word = ifelse(hunspell_check(word),word,hunspell_suggest(word)[[1]][1])
      }
      else {  
         # if the word is a slang/abb, then convert to english
         word = ifelse(has.key(word,h_slang),h_slang[[word]],word)
      
         # if the word is an emoticon, convert to english
         word = ifelse(has.key(word,h_emoticons),h_emoticons[[word]],word)
      }
    }
  }
}


#cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), stringsAsFactors = FALSE)
# posted comments on SO about this not working

tdm <- TermDocumentMatrix(tweet.corpus,
                          control = list(
                                         removePunctuation = TRUE,
                                         stopwords = TRUE,
                                         stem_words("english")))


pos.score <- tm_term_score(tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

neg.score <- tm_term_score(tdm,terms_in_General_Inquirer_categories("Negativ")) 


cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), 
                             positive = pos.score, 
                             negative = neg.score, 
                             stringsAsFactors = FALSE)



cleaned_tweets <- cleaned_tweets %>% 
  mutate(rown = row_number(), 
         positivity = (positive - negative) / (positive + negative + 1))


cleaned_tweets <- 
  cleaned_tweets %>% 
  inner_join(train.data.new)

save(cleaned_tweets,file="../data/cleaned_tweets.Rda")

```

