# Data cleaning

## Read the data

In the following, we define a function that reads the corpus from [Nikfarjam et al]: for each tweet it contains detailed information about the adverse event. 

The steps consists of

1. Parse the annotation file and label the columns
2. Read the dataset obtained from executing the Python script mentioned in the previous section: 
3. Read the Adverse Drug reaction lexicon
4. combine all results

```{r 03_parse_diego_data}
#' Parse downloaded annotated data
#'
#' @description Parses the data from the DIEGO labs. Depending on the argument
#' it reads in the training or test data. It joins the tweets with the
#' annotations. Empty annotations means no adverse reaction.
#'
#' @param type = c("train", "test")
#'
#' @return data frame
#'
#' @examples parse_diego_data("train")
#' @import dplyr, magrittr, readr, stringr

parse_diego_data <- function(type) {
    # Load data
    annotation_file <-
        ifelse(type == "train",
               "../data/download_tweets/train_tweet_annotations.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/test_tweet_annotations.tsv",
                      stop("type should be either 'train' or 'test'"
                      ))
               )

    tweets_file <-
        ifelse(type == "train",
               "../data/download_tweets/full_train_tweet_ids.tsv",
               ifelse(type == "test",
                      "../data/download_tweets/full_test_tweet_ids.tsv",
                      stop("type should be either 'train' or 'test'"))
               )


    # Read annotations and rename columns
    annotations <-
        read_tsv(annotation_file,
                 col_names = FALSE,
                 col_types = c("ciicccc")
        ) %>%
        rename(text_id = X1,
               start_offset = X2,
               end_offset = X3,
               semantic_type = X4,
               annotated_text = X5,
               related_drug = X6,
               target_drug = X7
        )

    # Parse tweets
    tweets <-
        read_lines(tweets_file) %>%
        str_split(., "\t") %>%
        as.data.frame() %>%
        as.matrix() %>%
        t %>%
        as.data.frame() %>%
        `rownames<-`(., NULL) %>%
        rename(tweet_id = V1,
               user_id = V2,
               text_id = V3,
               tweet_text = V4
        )

    # Read in the ADR lexicon and rename
    adr_lexicon <- read_tsv("../data/download_tweets/ADR_lexicon.tsv",
                            skip = 21,
                            col_names = FALSE) %>%
        rename(concept_id = X1,
               concept_name = X2,
               source = X3)

    # Join tweets and annotations
    result <-
        suppressWarnings(left_join(tweets, annotations, by = "text_id"))

    # Join result with ADR lexicon
    result <- left_join(result,
                        adr_lexicon,
                        by = c("annotated_text" = "concept_name"))

    # Select distinct tweets
    result <- result %>% distinct(tweet_id)

    result[ ] <- lapply(result, as.character)

    result
}
```

Next, we define a similar function that reads in the data from [Abeed Sarker and Graciela Gonzalez]. Reading this dataset is easier and just consists of reading the dataset obtained from executing the Python script.

```{r 03_parse_binary_annotations}
#' Parse binary data
#'
#' @description Parses the data with binary annotationÂ¨
#'
#' @return data frame
#'
#' @examples parse_binary_data()
#' @import dplyr, purrr, readr, stringr

parse_binary_data <- function(type) {
    # Load data
    tweets_file <- "../data/download_tweets/binary_tweets_downloaded.tsv"

    # Parse tweets: bad character "\r" has to be removed before the string
    # can be split
    individual_tweets <-
        read_file(tweets_file) %>%
        str_replace_all("\r", "") %>%
        str_split(., "\n") %>%
        unlist()

    # Split the tweets and have four output columns
    # Revalue is_AE column (needed for caret to work with some ML algos)
    tweets_frame <-
        individual_tweets %>%
        str_split_fixed("\t", 4) %>%
        as.data.frame() %>%
        rename(tweet_id = V1,
               user_id = V2,
               is_AE = V3,
               tweet_text = V4
        ) %>%
        filter(is_AE != "") %>%
        mutate(is_AE = as.factor(plyr::revalue(is_AE,
                                               c("0" = "No", "1" = "Yes"))
                                 ))
    tweets_frame[ ] <- lapply(tweets_frame, as.character)

    tweets_frame
}
```

Now that we have defined the functions reading in the data, let us take a first look at them to get a feeling.

## First look at the data

```{r 03_first_look_at_data}
# Read in the training data
train.data <- parse_diego_data("train")
test.data <- parse_diego_data("test")
binary.data <- parse_binary_data()
```

We can see from the tweets that we might want to extract the drug names "humira" to use as a feature in future models. Even though the researchers have annotated the drug names on this dataset, this should help us classify non-annotated tweets. This could serve the puropse of **automatically** creating new training sets:

If we want to predict ADRs on new drugs, our trained model should not have the drug name we are interested in included as feature. For the purpose of generating a new training sample, this could, however, be a good strategy, although one has to be careful with bias.

```{r 03_example_1}
train.data$tweet_text[709] 
```

Although our sample of tweets, for the most part, were correctly spelt, we still wanted to run it through a spell checker to ensure our models were robust.

```{r 03_example_2}
train.data$tweet_text[318]
```

We can also see a few examples of internet slang usage. This is hard for a spell checker to correct so we want to investigate if we can clean this up a bit.

```{r 03_example_3}
train.data$tweet_text[751]
```

A further challenge we see is that there is sometimes repeats of characters to show emphasis. This will need to be corrected also.

```{r 03_example_4}
train.data$tweet_text[807]
```

## Drug names

The drug names were downloaded from [here](https://www.nlm.nih.gov/research/umls/rxnorm/docs/rxnormfiles.html).

These are stored as `./data/dictionaries/RXNCONSO.RFF`.

We create a function that makes a list of drugs so we can identify tweets containing a known drug name, as well as ignore them from spell checking. We restrict to brand names (BN) and generics / ingredients (IN)

```{r 03_create_drug_list}
make_drug_list <- function() {
    # Read drug names
    rx.conso <- read.delim(file = '../data/dictionaries/RXNCONSO.RRF',
                       sep='|',
                       header = F,
                       stringsAsFactors = F)

    # Column names
    rx.colstr <- "RXCUI LAT TS LUI STT SUI SPREF RXAUI SAUI SCUI SDUI SAB TTY CODE STR SRL SUPPRESS CVF"

    # Assign column names to data frame
    names(rx.conso) <- rx.colstr %>%
        tolower %>%
        strsplit(split=' ') %>%
        unlist

    # Subset a list of drug brand names (BN) and generics/ingredient name (IN)
    # convert to lower for easier lookup
    drug.list <- rx.conso %>%
        subset((tty %in% c('BN','IN'))) %>%
        select(tty, code, str) %>%
        mutate(str = tolower(str))

    drug.list
    }
```



## Slang words

The idea in this step is to clean slang expression and convert then to proper english as well as possible. This is done before passing the tweets to the spellchecker.

Slang words were scraped from [here](http://www.netlingo.com/acronyms.php).

Again, we write a function that makes the conversion table:

```{r 03_slang_expressions}
make_slang_lookup <- function() {
    url <- "http://www.netlingo.com/acronyms.php"

    contents <- read_html(url)

    #write(contents, file = "../data/dictionaries/slang.txt")

    #contents <- read.table("../data/dictionaries/slang.txt")

    #lapply(contents, write, "../data/dictionaries/slang_words", append=TRUE, ncolumns=1000)

    # Using the chrome selector gadget we identified the following attributes containing the dictionary: .list_box3 li, .list_box3 span
    words <- contents %>%
        html_nodes(".list_box3 span") %>%
        html_text() %>% tolower

    descriptions <- contents %>%
        html_nodes(".list_box3 li") %>%
        html_text() %>% tolower

    slang.lookup <- data.frame(words, descriptions, stringsAsFactors = FALSE)

    # Cleaning out the descriptions, as it contains the slang words also
    # Also removing slangs that can have multiple interpretations
    # Remove the null key
    slang.lookup <- slang.lookup %>%
        mutate(decode = substring(descriptions,nchar(words)+1)) %>%
        filter(!grepl(",|-or-",descriptions)) %>%
        filter(words != "")
}
```

Next, we put every together: we add new attributes to the dataset

+ We extract the **hastags**
+ We see whether the tweet text contains a **URL**. This could be related to advertisments
+ Add positive and negative sentiments
+ Stem the text
+ Extract drugs

```{r 03_clean_tweets, eval = FALSE}
clean_tweets <- function(train.data, drug.list, slang.lookup) {

  #Loading the emoticon dataset from qdapDictionaries. This converts emoticons to their english text counterpart
  data("emoticon")

  train.data.new <- train.data %>%
    mutate(
      #ae_term       = substr(tweet_text,start_offset,end_offset),      # Extract ae terms (only applicable here)
      hashtag       = str_extract_all(tweet_text,"#[a-zA-Z]+"),        # Collect all hashtags
      URL           = grepl("http[[:alnum:][:punct:]]*", tweet_text)   # Boolean to indicate if tweet contains UR
      
    ) %>%
    arrange(tweet_id) %>%
    mutate(rown = row_number())
  
  
  # Reshaping the data
  # Transposing to get one word per row using unnest without removing punctuation. This allows us to preserve emoticons
  train.data.long <- train.data.new %>%
    select(tweet_id, tweet_text) %>%
    mutate(words = strsplit(as.character(tolower(tweet_text))," ")) %>%
    unnest() %>%
    group_by(tweet_id) %>%
    mutate(wordn = row_number()) %>%
    ungroup %>%
    left_join(emoticon, by=c("words"="emoticon"))             # Add the meaning for the emoticons from the emoticon dictionary


  train.data.aug <- train.data.long %>%
    mutate(
      is_hashtag = grepl("#[a-zA-Z]+",words),                 # attribute to show if its a hashtag
      is_URL     = grepl("http[[:alnum:][:punct:]]*",words),  # is it a url
      words      = gsub("([:lower:])\\1+","\\1\\1", words),   # Contract word length
      words      = gsub("[[:punct:]]", "", words)             # Remove punctuation
      ) %>%
    filter(words != "") %>%
    left_join(mutate(drug.list, drug_name=str), by=c("words"="str")) %>%
    left_join(slang.lookup, by="words")



  # this part runs really slow, be warned
  train.data.clean <- train.data.aug %>%
    rowwise() %>%
    mutate(
      words = ifelse((is.na(drug_name) && !is_hashtag && !is_URL),
             ifelse (hunspell_check(words),
                     words,
                     ifelse(is.na(decode),
                            hunspell_suggest(words)[[1]][1],
                            decode 
                            )
                     ),
             words
             )
    )
  
  # aggregating the entire tweets back to one line per tweet
  train.data.wide <- train.data.clean%>%
    group_by(tweet_id) %>%
    summarise(cleaned_tweet = paste(words, collapse = " "))


  # Creating a corpus to create the term document matrix
  tweet.corpus <- Corpus(VectorSource(train.data.wide$cleaned_tweet))

  # Stem the words
  tweet.corpus <- tm_map(tweet.corpus, stemDocument)
  
  # Create the term document matrix, removing punctuation and stopwords (common words that carry little meaning)
  tdm <- TermDocumentMatrix(tweet.corpus,
                            control = list(
                                           removePunctuation = TRUE,
                                           stopwords = TRUE))

  
  # Generating a sentiment score to be used as a feature in the predictive model
  pos.score <- tm_term_score(tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

  neg.score <- tm_term_score(tdm,terms_in_General_Inquirer_categories("Negativ"))


  cleaned_tweets <- data.frame(stemmed = sapply(tweet.corpus, as.character),
                               positive = pos.score,
                               negative = neg.score,
                               stringsAsFactors = FALSE)

  # Sentiment scoring
  cleaned_tweets <- cleaned_tweets %>%
    mutate(rown = row_number(),
           positivity = (positive - negative) / (positive + negative + 1))

  # merge this onto the original tweet dataset that other features for the predictive model
  cleaned_tweets <-
      cleaned_tweets %>%
      inner_join(train.data.new, by="rown")
  
  #Remove drugs from text
  drugs <-
      read_lines("../data/download_tweets/drug_names.txt", skip = 6) %>% 
      stemDocument()
  drug_regex <- drugs %>% str_c(collapse = "|") %>% str_c("(", ., ")")
  
  # Remove hashtag symbol
  cleaned_tweets$hashtag %<>% str_replace_all("#", "")
  
  # Change hashtags
  cleaned_tweets$hashtag %<>% str_to_lower() %>% stemDocument()
  cleaned_tweets$hashtag <- plyr::revalue(cleaned_tweets$hashtag, c("character(0)" = NA))
  
  # Extract drug
  cleaned_tweets$drug <- 
      str_c(str_extract_all(cleaned_tweets$stemmed, drug_regex),
            str_extract_all(cleaned_tweets$hashtag, drug_regex)) %>% 
      str_replace_all("character\\(0\\)", "")
  
  # Remove drug from tweet and hashtag
  cleaned_tweets$stemmed %<>% str_to_lower %>%  str_replace_all(drug_regex, "")
  
  cleaned_tweets$hashtag %<>% 
      str_replace_all(drug_regex, "") %>% 
      str_replace_all('", ', "") %>% 
      str_replace_all('c\\("', "") %>% str_replace_all('"\\)', "") %>%
      str_replace_all('\\"', " ") %>% 
      ifelse(. == "", NA, .)
  
  cleaned_tweets
}

cleaned_train <- clean_tweets(train.data, make_drug_list(), make_slang_lookup())
cleaned_test <- clean_tweets(test.data, make_drug_list(), make_slang_lookup())
cleaned_binary <- clean_tweets(binary.data, make_drug_list(), make_slang_lookup())

save(cleaned_train,file="../data/cleaned_train.Rda")
save(cleaned_test,file="../data/cleaned_test.Rda")
save(cleaned_binary,file="../data/cleaned_binary.Rda")
```
