# Data collection

## Data used in previous studies

Twitter Corpus was downloaded from [here](http://diego.asu.edu/Publications/ADRMine.html) and [here](http://diego.asu.edu/downloads/twitter_annotated_corpus/).

The first is the Corpus referred to in the paper by [Nikfarjam et al: 
Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features](http://www.ncbi.nlm.nih.gov/pubmed/25755127) and the second one comes from [Abeed Sarker and Graciela Gonzalez. Portable automatic text classification for adverse drug reaction detection via multi-corpus training. Journal of Biomedical Informatics. 53 (2015) 196-207.](http://diego.asu.edu/Publications/ADRClassify.html)

The first corpus contains the following files:
  - Test tweet annotations
  - Test tweet ids
  - Train tweet annotations
  - Train tweet ids
  - Program to download tweets
  
The second corpus consists of two files:
  - Tweet ids
  - Program to download tweets

For data confidentiality reasons, the researchers had not made the individual tweets available. However, they had provided a python script that would retreive the Tweets from the tweed id. To maintain the confidentiality, we have made our github area private.

The download of the dataset is not executed by default.

```{r 02_download_data, eval = FALSE}
# Download dataset; not executed by default
system("python ../data/download_tweets/download_tweets.py ../data/download_tweets/train_tweet_ids.tsv ../data/download_tweets/full_train_tweet_ids.tsv")
system("python ../data/download_tweets/download_tweets.py ../data/download_tweets/test_tweet_ids.tsv ../data/download_tweets/full_test_tweet_ids.tsv")
system("python ../data/download_tweets/download_tweets.py ../data/download_tweets/Twitter_corpus_releaseset_external.txt ../data/download_tweets/binary_tweets_downloaded.tsv")
```

## Script for collection new data

In the following, we download tweets containing the drug names listed below. The data collection was performed on April 21.

```{r 02_collect_new_tweets, eval = FALSE}
setup_twitter_oauth("consumer_key", "consumer_secret", 
                    "access_token", "access_secret")

drugs<-c("humira", 
         "dronedarone",
         "lamictal",
         "pradaxa",
         "paxil",
         "zoledronic acid",
         "trazodone",
         "enbrel",
         "cymbalta",
         "quetiapine",
         "cipro",
         "lozenge",
         "dabigatran",
         "olanzapine",
         "fluoxetine",
         "vyvanse",
         "seroquel",
         "fosamax",
         "paroxetine",
         "nicotine",
         "effexor",
         "prozac",
         "tysabri",
         "rivaroxaban",
         "baclofen",
         "lamotrigine",
         "venlafaxine",
         "apixaban",
         "avelox",
         "levaquin",
         "zyprexa",
         "duloxetine",
         "ofloxacin",
         "geodon",
         "victoza",
         "metoprolol",
         "viibryd",
         "pristiq",
         "nesina",
         "factive",
         "gamma-aminobutyric acid",
         "sabril",
         "livalo",
         "denosumab",
         "bystolic",
         "xarelto",
         "floxin",
         "boniva",
         "saphris",
         "ziprasidone",
         "memantine",
         "namenda",
         "latuda",
         "fycompa",
         "canagliflozin",
         "zometa",
         "etanercept",
         "lurasidone",
         "alendronate",
         "linagliptin",
         "effient",
         "vimpat",
         "eliquis",
         "liraglutide",
         "pregabalin",
         "onglyza",
         "nicotrol inhaler",
         "lyrica",
         "invokana",
         "commitlozenge",
         "actonel",
         "nicotrolinhaler",
         "synthroid",
         "albuterol",
         "nasonex",
         "spiriva",
         "suboxone",
         "nexium",
         "januvia",
         "valsartan",
         "tamiflu"
         )

tweets = list()
for(i in 1:length(drugs)) {
  result <- searchTwitter(drugs[i],
                          n = 1500,
                          lang = "en",
                          since = "2016-01-01")
  Sys.sleep(60)
  tweets <- c(tweets,result)
  tweets <- unique(tweets)
}

save(tweets, file = "../data/download_tweets/datacollection.RData")
```

To be able to manually annotate some of the new tweets, we converted them into tabular form: a data frame having two columns:

+ tweet_id
+ tweet_text

Furthermore, we saved a sample of 400 tweets as an Excel file for easier annotation.

```{r 02_save_new_data_excel, eval = FALSE}
set.seed(123)
parse_new_tweets <- function(tweets) {
    tweets_text <- tweets %>% map_chr(~.$text)
    tweets_id <- tweets %>% map_chr(~.$id)
    
    data_frame(tweet_id = tweets_id,
               tweet_text = tweets_text)
}

write.xlsx(parse_new_tweets(tweets) %>% sample_n(400),
           file = "../data/download_tweets/tweets_for_annotation.xlsx")
```

Due to problems with encoding, we had to erase 5 tweets from the Excel file leaving us with 395 tweets for manual annotation.