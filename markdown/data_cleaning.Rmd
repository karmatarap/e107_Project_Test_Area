---
title: "data_cleaning"
author: "Karma Tarap"
date: "14 April 2016"
output: html_document
---

#Data collection

Twitter Corpus was downloaded from: http://diego.asu.edu/Publications/ADRMine.html

This is the Corpus referred to in the paper by Nikfarjam et al: 
Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features

The corpus contains the following files:
  - Test tweet annotations
  - Test tweet ids
  - Train tweet annotations
  - Train tweet ids
  - Program to download tweets
  
For data confidentiality reasons, the researchers had not made the individual tweets available. However, they had provided a python script that would retreive the Tweets from the tweed id. To maintain the confidentiality, we have made our github area private.

The Tweets were downloaded using the following commands:
  python download_tweets.py test_tweet_ids.tsv full_test_tweet_ids.tsv
  python download_tweets.py test_train_ids.tsv full_train_tweet_ids.tsv
  
  
  

```{r}

setwd("/Users/bear/Studies/Harvard/Data_Science/project/e107_Project_Test_Area/markdown")

# Read in the training data 
source("../r/helper_functions/parse_diego_data.R")

train.data <- parse_diego_data("train")

#converting factors to string
train.data[] <- lapply(train.data, as.character)


# Libraries used
library(qdap)
library(qdapDictionaries)
library(rafalib)
library(dplyr)
library(tm)
library(stringr)
library(rvest)    #web scraping
library(hunspell) #spelling dictionary, installed with brew on MacOS
library(hash)
set.seed(999)

# Investigate a subsection of the data to see what kind of text we are dealing with 
x <- train.data[sample(nrow(train.data),5), ]
x
```

# Drug names

## Downloaded drug names from https://www.nlm.nih.gov/research/umls/rxnorm/docs/rxnormfiles.html

These are stored as RXNCONSO.RFF. 
Create a list so we can identify tweets containing a known drug name, as well as ignore them from spell checking


```{r}

# Read drug names 
rx.conso <- read.delim(file = '../data/dictionaries/RXNCONSO.RRF', sep='|',header = F, stringsAsFactors=F)


# Column names
rx.colstr <- "RXCUI LAT TS LUI STT SUI SPREF RXAUI SAUI SCUI SDUI SAB TTY CODE STR SRL SUPPRESS CVF"

# Assign column names to data frame
names(rx.conso) <- rx.colstr %>%
  tolower %>%
  strsplit(split=' ') %>% 
  unlist

# Subset a list of drug brand names and generics
# convert to lower for easier lookup
drug.list <- rx.conso %>%
  subset((tty %in% c('BN','IN'))) %>%
  select(tty,code,str) %>%
  mutate(str=tolower(str))


```


# Slang words

Scrape these from http://www.netlingo.com/acronyms.php.

Identify a list of slang words, so we can translate them into their english counterpart before passing to the spellchecker 

```{r}


#url <- "http://www.netlingo.com/acronyms.php"

#contents <- read_html(url)

write(contents, file = "../data/dictionaries/slang.txt")

contents <- read.table("../data/dictionaries/slang.txt")

lapply(contents, write, "../data/dictionaries/slang_words", append=TRUE, ncolumns=1000)

# Using the chrome selector gadget we identified the following attributes containing the dictionary: .list_box3 li, .list_box3 span
words <- contents %>%
  html_nodes(".list_box3 span") %>%
  html_text() %>% tolower 

descriptions <- contents %>%
  html_nodes(".list_box3 li") %>%
  html_text() %>% tolower 

slang.lookup <- data.frame(words, descriptions, stringsAsFactors = FALSE)

# Cleaning out the descriptions, as it contains the slang words also
# Also removing slangs that can have multiple interpretations
# Remove the null key
slang.lookup <- slang.lookup %>% 
  mutate(decode = substring(descriptions,nchar(words)+1)) %>% 
  filter(!grepl(",|-or-",descriptions)) %>%
  filter(words != "")


```







```{r}

# Adding some attributes to the training dataset


train.data.new <- train.data %>% 
  mutate(vect    = strsplit(tweet_text,split=" "), 
         hashtag = str_extract_all(tweet_text, "#[a-zA-Z]+"),
         URL     = grepl("http[[:alnum:][:punct:]]*", tweet_text),
         ae_term = substr(tweet_text,start_offset,end_offset)) 


# get drug names
drugs <- sapply(train.data.new$vect,function(x){intersect(x,drug.list$str)})

# get list emoticon meanings
data("emoticon")
emoticons <- sapply(train.data.new$vect, function(x){emoticon$meaning[match(x,emoticon$emoticon)]})

# retrieve drug names and emotions
train.data.new <- train.data.new %>% 
  mutate(rown = row_number(),
         drugs = drugs,
         emoticons = emoticons)

#place emoticons and slangs into a lookup for substitution
h_emoticons <- hash(emoticon$emoticon, emoticon$meaning)
h_slang <- hash(slang.lookup$words, slang.lookup$decode)


# Create corpus
tweet.corpus <- Corpus(VectorSource(train.data.new$tweet_text))

# Convert all to lower case
tweet.corpus <- tm_map(tweet.corpus, content_transformer(tolower))

# Create some functions to help with the cleaning
# Coding urls to url so we can group them and use as a feature in our model
subURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "url", x)
subURL("ofdmfl http://dfdsofdf") == "ofdmfl url"

# Remove twitter handles references
removeHandles <- function(x) gsub("@[[:alnum:][:punct:]]+", "", x)
removeHandles("@so_fd45 this is a test") == " this is a test"

# No words have three consecutive letters of the same character, so compressing to 2
contractLength <- function(x) gsub("([:lower:])\\1+","\\1\\1", x)
contractLength("coooool") == "cool"


# Start by contracting the length
tweet.corpus <- tm_map(tweet.corpus, content_transformer(contractLength))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(removeHandles))
tweet.corpus <- tm_map(tweet.corpus, content_transformer(subURL))


for (tweet in seq(tweet.corpus)){
  for (word in str_split(tweet.corpus[[tweet]],"[^a-z0-9_]")[[1]]){
    
    # ignore if the word is a drug
    if (!word %in% drug.list$str && word !=""){
      
      if (!has.key(word,h_slang) && !has.key(word,h_emoticons)){
         word = ifelse(hunspell_check(word),word,hunspell_suggest(word)[[1]][1])
      }
      else {  
         # if the word is a slang/abb, then convert to english
         word = ifelse(has.key(word,h_slang),h_slang[[word]],word)
      
         # if the word is an emoticon, convert to english
         word = ifelse(has.key(word,h_emoticons),h_emoticons[[word]],word)
      }
    }
  }
}


#cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), stringsAsFactors = FALSE)


# install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) # posted comments on SO about this not working

tdm <- TermDocumentMatrix(tweet.corpus,
                          control = list(
                                         removePunctuation = TRUE,
                                         stopwords = TRUE,
                                         stem_words("english")))


pos.score <- tm_term_score(tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

neg.score <- tm_term_score(tdm,terms_in_General_Inquirer_categories("Negativ")) 


cleaned_tweets <- data.frame(cleaned = sapply(tweet.corpus, as.character), 
                             positive = pos.score, 
                             negative = neg.score, 
                             stringsAsFactors = FALSE)



cleaned_tweets <- cleaned_tweets %>% 
  mutate(rown = row_number(), 
         positivity = (positive - negative) / (positive + negative + 1))


cleaned_tweets <- 
  cleaned_tweets %>% 
  inner_join(train.data.new)

save(cleaned_tweets,file="../data/cleaned_tweets.Rda")

```

