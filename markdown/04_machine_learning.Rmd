# Predicting adverse events

In this section, we train a machine learning with the goal of being able to predict adverse events from tweets. In a second step, we apply this machine learning to recent twitter data. 

The question we ask here is the following:

> If we train a model for predicting adverse drug reaction from Twitter data generated in 2013, are the results still valid nowadays?

This question is of critical importance to the research program initiated by the papers mentioned in our text: if the language in social media changes so drastically over few years that models trained on old data are not valid anymore, this means that experts have to annotate quite large amounts of tweets on a regular basis to have valid training samples.

A more refined version of the question above is:

> Are the quantitative measures "precision", "recall", "F-score" and "auc" when the model is evaluated on recent data significantly lower than on the validation set used in the original studies?

A quantitative way of evaluating this question is to train several models on the older dataset and compare the various scores when evaluated on new data.

## Data

The dataset was prepared in the Data Cleaning section. Here, we combine the various datasets and split into a training and validation set:

+ The dataset with detailed annotation comes split into a training and a test set. We use this splitting.
+ The binary annotated dataset is split into a training (75%) and cross-validation (25%) set

The reason for actually splitting the dataset at this stage and not just performing repeated cross validation is that we will train **several models** on the training data and then [use the scores of these models as **inputs** to a new machine learning model](http://www.stat.berkeley.edu/~ledell/docs/dlab_ensembles.pdf). So, in order to perform proper cross validation we should repeat this entire procedure several times.

```{r 03_read_data, warning = FALSE}
tidy_data <- function() {
    # Prepares two tidy datasets: training set and test set
    #
    # Args: None
    #
    # Returns: list whose first entry is the training set and whose second
    #          entry is the test set
    
    # Load datasets
    load("../data/cleaned_train.Rda")
    load("../data/cleaned_test.Rda")
    load("../data/cleaned_binary.Rda")
    
    train_data_annotated <- 
        cleaned_train %>%
        mutate(is_AE = as.factor(ifelse(!is.na(start_offset), "Yes", "No")))
    
    test_data_annotated <-
        cleaned_test %>%
        mutate(is_AE = as.factor(ifelse(!is.na(start_offset), "Yes", "No")))
    
    # Read binary annotated dataset
    binary_data <- cleaned_binary
    binary_train_ind <- createDataPartition(binary_data$is_AE, p = 0.75, list = FALSE)
    
    # Bind two sets
    train_data <-
        bind_rows(train_data_annotated,
                  binary_data %>% slice(binary_train_ind)
                  ) %>% 
        select(tweet_id, user_id, stemmed, positive, 
               negative, hashtag, URL, is_AE) %>% 
        distinct(tweet_id)
    
    test_data <- 
        bind_rows(test_data_annotated,
                  binary_data %>% slice(-binary_train_ind)
                  )  %>% 
        select(tweet_id, user_id, stemmed, positive, 
               negative, hashtag, URL, is_AE)
    
    train_data$is_AE %<>% as.factor
    test_data$is_AE %<>% as.factor
        
    # Remove drugs from text
    drugs <-
        read_lines("../data/download_tweets/drug_names.txt", skip = 6) %>% 
        stemDocument()
    drug_regex <- drugs %>% str_c(collapse = "|") %>% str_c("(", ., ")")
    
    # Remove hashtag symbol
    train_data$hashtag %<>% str_replace_all("#", "")
    test_data$hashtag %<>% str_replace_all("#", "")
    
    # Change hashtags
    train_data$hashtag %<>% str_to_lower() %>% stemDocument()
    test_data$hashtag %<>% str_to_lower() %>% stemDocument()
    
    train_data$hashtag <- plyr::revalue(train_data$hashtag, c("character(0)" = NA))
    test_data$hashtag <- plyr::revalue(test_data$hashtag, c("character(0)" = NA))
    
     # Extract drug
    train_data$drug <- 
        str_c(str_extract_all(train_data$stemmed, drug_regex),
              str_extract_all(train_data$hashtag, drug_regex)) %>% 
        str_replace_all("character\\(0\\)", "")
    test_data$drug <- 
        str_c(str_extract_all(test_data$stemmed, drug_regex),
              str_extract_all(test_data$hashtag, drug_regex)) %>% 
        str_replace_all("character\\(0\\)", "")
    
    # Remove drug from tweet and hashtag
    train_data$stemmed %<>% str_to_lower %>%  str_replace_all(drug_regex, "")
    test_data$stemmed %<>% str_to_lower %>%  str_replace_all(drug_regex, "")
    
    train_data$hashtag %<>% 
        str_replace_all(drug_regex, "") %>% 
        str_replace_all('", ', "") %>% 
        str_replace_all('c\\("', "") %>% str_replace_all('"\\)', "") %>%
        str_replace_all('\\"', " ") %>% 
        ifelse(. == "", NA, .)
    
    test_data$hashtag %<>% 
        str_replace_all(drug_regex, "") %>% 
        str_replace_all('", ', "") %>% 
        str_replace_all('c\\("', "") %>% str_replace_all('"\\)', "") %>%
        str_replace_all('\\"', " ") %>% 
        ifelse(. == "", NA, .)
    
    train_data %<>% distinct(stemmed)
    
    list(train = train_data, test = test_data)
}

data <- tidy_data()
```

The dataset consists at this stage of the following features:

Column name      |  Description
-----------------|-------------
tweet_id         | Unique identifier of the tweet
user_id          | Unique identifier of the user posting the tweet
stemmed          | Actual tweet text, cleaned and stemmed
positive         | Positive sentiments
negative         | Negative sentiments
hastag           | character of hashtags
URL              | does the tweet contain a URLS
is_AE            | Is the tweet related to an adverse event
drug             | names of drugs in tweet


## Exploratory data analysis

Let us first do exploratory data analysis on the tweets.

The first interesting question we have is how **balanced** the dataset is: what is the proportion of adverse event related tweets?

```{r 03_explore_data_imbalance}
data[[1]] %$%
    table(is_AE) %>% 
    pander
```

So there is quite a big imbalance: most tweets containing one of the drug names are not related to an adverse event.

One very popular method for this is to use **wordclouds**. We first produce two wordclouds: one (in black) with tweets not related to an AE and one (in green) related to adverse events. We see that there are interesting differences between the two wordclouds.

+ The right one contains many more "negative" words and words that are obviously related to not feeling good
+ The left wordcloud contains many neutral words, but also many negative words. These negative words, however are not related to an adverse event (consider, e.g. the phrase "depression hurts cymbalta can help")

```{r 03_wordcloud_overall, warning = FALSE}
par(mfrow = c(1, 2))
wordcloud(data[["train"]] %>% filter(is_AE == "No") %$% stemmed, 
          min.freq = 20,
          random.color = TRUE, 
          colors = "black"
          )

wordcloud(data[["train"]] %>% filter(is_AE == "Yes") %$% stemmed, 
          min.freq = 20,
          random.color = TRUE, 
          colors = "green"
          )
```

```{r 03_word_counts}
data[[1]] %>% 
    unnest_tokens(word, stemmed) %>% 
    anti_join(stop_words, by = "word") %>% 
    count(word) %>% 
    top_n(30, n) %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    xlab("Word") +
    ylab("Count")
```

```{r 03_word_counts_comparions}
data[[1]] %>% 
    unnest_tokens(word, stemmed) %>% 
    anti_join(stop_words, by = "word") %>%
    filter(is_AE == "Yes") %>% 
    count(word) %>% 
    top_n(30, n) %>% 
    ungroup() %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    xlab("Word") +
    ylab("Count")
```

We see quite a big difference in sentiments between tweets corresponding to adverse events and those not corresponding to adverse events:

+ *Trust* is much lower in AE tweets
+ *Negative* is much lower in AE tweets
+ *Positive* sentiments occur with higher frequency in non-AE tweets

```{r 04_sentiments}
data[[1]] %>% 
    unnest_tokens(word, stemmed) %>% 
    inner_join(sentiments, by = "word") %>% 
    count(is_AE, sentiment) %>% 
    group_by(is_AE) %>% 
    mutate(n = n / n()) %>% 
    ggplot(aes(x = sentiment, y = n)) +
    geom_bar(stat = "identity") +
    facet_wrap(~is_AE) +
    coord_flip() +
    xlab("Sentiment") +
    ylab("Count")
```

We also see that tweets correspoding to AEs tend to be slightly longer, though that criterion alone is of course insufficient (this hold true even more considering the imbalance of the dataset)

```{r 04_tweet_length}
data[[1]] %>% 
    unnest_tokens(word, stemmed) %>% 
    count(tweet_id, is_AE) %>% 
    ggplot(aes(x = n, color = is_AE)) +
    geom_density() +
    xlab("Number of words per tweet") +
    scale_color_manual(values = c("Blue", "Red"),
                       name = "AE",
                       labels = c("No", "Yes"))
```


```{r 04_ngrams}
data[[1]] %>% 
    unnest_tokens(word, stemmed, token = "regex", "[a-z]{3}") %>% 
    count(tweet_id, is_AE) %>% 
    ggplot(aes(x = n, color = is_AE)) +
    geom_density() +
    xlab("Number of words per tweet") +
    scale_color_manual(values = c("Blue", "Red"),
                       name = "AE",
                       labels = c("No", "Yes"))
```


## Modeling
## Documenttermmatrix

The first step consists in preparing the **document term matrix** (DTM). This is a matrix whose columns corresponds to features of the text. 

+ Each word stem gives a feature, i.e. the feature matrix will have as many columns as there are distinct word stems in the corpus
+ The value of each feature for each document (tweet) can be computed in various ways:
  - one-hot-encoding: it is one if the word stem is in the tweet, 0 else
  - Term frequency: the value of the feature is the number of times the word stem has occured in the tweet
  - Term frequency inverse document frequency (*TfIdf*): this is a measure for the relative importance of the feature taking into account the frequency of the word stem in the entire corpus. For example, the word **the** may occur very frequently in a tweet, but it occurs also very often in the corpus. Thus its TfIdf will be low. On the other hand, if a word occurs often within a given tweet, but very rarely in the entire document, this means that the tweet is likely to be about this word and thus it obtains a high TfIdf.
  
We chose *Term Frequency* here to build the feature vectors. The reason is simple: the other approaches were tried, but using cross validation, Tf performed best.

```{r 03_create document matrix}
doc_matrix <- create_matrix(c(data[["train"]]$stemmed, 
                              data[["test"]]$stemmed),
                            language = "english",
                            removeNumbers = TRUE,
                            removePunctuation = TRUE,
                            toLower = TRUE,
                            stemWords = TRUE,
                            stripWhitespace = TRUE,
                            ngramLength = 1,
                            minDocFreq = 1,
                            weighting = tm::weightTf)
```

## Container

Now that we have a DTM, we could use it directly as a feature matrix and train a model using, e.g. the `caret` package. The package `RTextTools`, however, has a very convenient datastructure allowing us to put all important data into one **container**. In the folowing code chunk, we put the tweets and the outcome in the container, specifying which data are training data and which data are test data. The option `virgin = FALSE` specifies that we have full information about the outcomes on the test data.

```{r 03_create contained}
container <- 
    create_container(doc_matrix, 
                     c(data[["train"]]$is_AE, data[["test"]]$is_AE),
                     trainSize = 1:nrow(data[["train"]]),
                     testSize = 1:nrow(data[["test"]]) + nrow(data[["train"]]),
                     virgin = FALSE)
```

## Model training

Using the container described above, we can directly train models. Here, we train four different models (training the other models is quite slow and does not significantly improve the performance) using the standard parameters:

+ MAXENT: Maximum Entropy
+ GLMNET: this is a regularized version of logistic regression
+ SVM: regularized support vector machine
+ TREE: decision tree

```{r}
fit_models <-
    train_models(container,
                 c("MAXENT", "GLMNET", "SVM", "TREE"),
                 cost = 1000)
```

## Model prediction

The `RTextTools` package also allows us to make predictions quite easily. The following code chunk produces the results (i.e. predicted probability and predicted label) for each of the four models on the test set:

```{r}
preds <- classify_models(container, fit_models)
```

## Model evaluation

The next step is to evaluate the quality of the models. We start with AUC:

```{r}
par(mfrow = c(2, 2))
preds %$% roc(data[["test"]]$is_AE, MAXENTROPY_PROB) %>% plot
preds %$% roc(data[["test"]]$is_AE, GLMNET_PROB) %>% plot
preds %$% roc(data[["test"]]$is_AE, SVM_PROB) %>% plot
preds %$% roc(data[["test"]]$is_AE, TREE_PROB) %>% plot
```

We see that the regularized logistic regression performs best with quite a high area under the curve.

Next, the `RTextTools` package allows us to extract important other measures as well quite easily using the `create_analytics` command. This gives us the precision, recall and f-score for each of the models (recall that the f-score is a weighted average of the precision and the recall).

Also it automatically creates an **ensemble summary**. This is a useful statistics about the combined performance of all four models. The line corresponding to `n >= 2` means the following:

+ Coverage tells us on how many percent of the documents two ore more models agree.
+ The recall value corresponds to the recall statistics when restricted to those documents.

```{r}
analytics <- create_analytics(container, preds)
```

```{r}
summary(analytics)
```

```{r}
create_ensembleSummary(analytics@document_summary)
```


## Aggregating predictions:

The next step is to aggregate the predictions from different models into a final model.

```{r}
preds$results <- data[["test"]]$is_AE
tr_control <- trainControl(method = "cv",
                           number = 10,
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE)

tune_grid <- expand.grid(mtry = c(1, 2, 3, 4))

fit_rf<- train(results ~ MAXENTROPY_PROB + GLMNET_PROB + SVM_PROB + TREE_PROB,
               data = preds,
               method = "rf",
               trControl = tr_control,
               tuneGrid = tune_grid,
               metric = "ROC"
)
```

We look at the results:

```{r}
fit_rf
varImp(fit_rf)
```


The area under curve is around 85%. 

# Recent data

The next step consists in checking whether we can reproduce these results with recently collected twitter data.

```{r}
parse_new_tweets <- function() {
    # Prepares new tweets into a form that is suitable for being fed
    # into the classifiers
    #
    # Args: None
    #
    # Returns: data frame
    load("../data/download_tweets/datacollection.RData")
    tweets_text <- map_chr(tweets, ~.$text)
    
    # Remove drugs from text
    drugs <- read_lines("../data/download_tweets/drug_names.txt", skip = 6)
    drug_regex <- drugs %>% str_c(collapse = "|") %>% str_c("(", ., ")")
    
    tweets_text %<>% str_to_lower %>%  str_replace_all(drug_regex, "")

    tweets_text
}

new_tweets <- parse_new_tweets()
```

In order for the following code chunk to work, this [hack](https://groups.google.com/forum/embed/#!topic/rtexttools-help/Drqr3Z897Mk) has to be performed. Furthermore, we have to change the encoding to UTF-8.

```{r}
source("../r/helper_functions/create_matrix_fixed.R")
n_new_tweets <- 200
new_tweets_dtm <- 
    create_matrix(new_tweets %>% head(n = n_new_tweets),
                  originalMatrix = doc_matrix,
                  language = "english",
                  removeNumbers = TRUE,
                  removePunctuation = TRUE,
                  toLower = TRUE,
                  stemWords = TRUE,
                  stripWhitespace = TRUE,
                  ngramLength = 1,
                  minDocFreq = 1,
                  weighting = tm::weightTf)
```


```{r}
new_container <- create_container(new_tweets_dtm,
                                  labels = c(data[[1]]$is_AE, data[[2]]$is_AE),
                                  trainSize = NULL,
                                  testSize = 1:n_new_tweets,
                                  virgin = TRUE
                                  )
```

```{r}
preds_new <- classify_models(new_container, fit_models)
preds_new_final <- predict(fit_rf, preds_new, type = "prob")
```

Now we show the ten tweets for which the model predicts the highest probability of having an AE:

```{r}
order(preds_new_final[, 2], decreasing = TRUE) %>%
    head(n = 30) %>% 
    map_chr(~new_tweets[.])
```


## Save models

```{r 04_save_models}
save(fit_models, file = "../data/models/fit_models.RData")
save(fit_rf, file = "../data/models/stacked_rf.RData")
```

